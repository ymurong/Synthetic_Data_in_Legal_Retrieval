# Syntactical Structure Matters: Enhancing French Statutory Article Retrieval with GPT-Engineered Synthetic Questions

The goal of this thesis is to enhance the french statutory article retrieval performance through optimized query generation in order to decrease the reliance on human provided labelled examples. The collection of large-scale, high-quality annotated datasets especially in closed domains like legal text is not only time-consuming but also entails significant expenses.  This has constituted a significant bottleneck for various legal IR and natural language processing (NLP) tasks.

For this, we aim to explore the impact of syntactical structure differences of synthetic questions on legal domain adaptation and provide guidelines on how to use LLMs for better statutory retrieval performance in a semi-supervised way. The central research question that is answered during this research
is as follows:

The project aims to answer the following research question: 

> RQ: How do syntactical characteristics of synthetic legal questions generated by large language models (LLMs) influence the accuracy of statutory article retrieval, and based on this impact, what specific guidelines can be established for optimizing the use of LLMs in pseudo-labeling in legal information retrieval domain?

    * SRQ1 What are the syntactical structure differences between current synthetic questions and real human questions ? 

    * SRQ2 What are the prompting techniques that can reduce the syntactical structure differences ? 

    * SRQ3 To what extent can syntactical structure impact the accuracy of statutory article retrieval ?

    * SRQ4 What are the prompting techniques that improve the accuracy of statutory article retrieval?

The specific contributions of the proposed research are as follows:

(1) We provide optimized french synthetic queries for the statutory article dense retrieval task.

(2) We provide semi-supervised benchmark in the usage of different prompting techniques on the statutory article dense retrieval task. 

(3) We provide guidelines on synthetic query generation through LLMs prompting in legal information retrieval.

## Project Folder Structure

This project has the following folder structure:


## Project Setup

### Windows
First, you need to install dependencies.
```bash
python3 -m venv venv
source  venv/Scripts/activate
pip install -r requirements.txt
```

> For windows, wheel files here: https://pypi.tuna.tsinghua.edu.cn/simple or https://www.lfd.uci.edu/~gohlke/pythonlibs/

### Mac
First, you need to install dependencies.
```bash
python3 -m venv venv
source  venv/Scripts/activate
pip install -r requirements.txt
```

Secondly, some sub dependencies are needed.
```bash
python -m spacy download fr_core_news_md
```

## EDA
* [Exploratory Data Analysis (Before Experimentation)](scripts/eda/Exploratory_Data_Analysis(Before_Experimentation).ipynb)
* [(WIP) Exploratory Data Analysis (After Experimentation)](scripts/eda/Exploratory_Data_Analysis(After_Experimentation).ipynb)


## Experiments

### Baseline Reproduction

#### TF-IDF, BM25, FastText, Word2Vec
In order to reproduce the TF-IDF, BM25, FastText, Word2Vec results, run:
```bash
python scripts/baseline/bsard/experiments/run_zeroshot_evaluation.py \
    --retriever {tfidf, bm25, word2vec} \ 
    --lem true
```

####  GPL + mT5
In order to reproduce the GPL + mT5 results, run:
```python
# GPL Generating queries
import gpl
gpl.toolkit.qgen(
    data_path = "./scripts/gpl/data/bsard",
    output_dir = "/content/drive/MyDrive/UVA/Thesis/training/gpl/bsard/generated",
    generator_name_or_path="doc2query/msmarco-french-mt5-base-v1",
    ques_per_passage=1,
    bsz=2,
    qgen_prefix="qgen",
)

# Mining the similar but non relevant passage 
gpl.toolkit.NegativeMiner(
    generated_path = "/content/drive/MyDrive/UVA/Thesis/training/gpl/bsard/generated",
    prefix="qgen",
    retrievers=["antoinelouis/biencoder-camembert-base-mmarcoFR", "antoinelouis/biencoder-mMiniLMv2-L12-mmarcoFR"],
    retriever_score_functions=["cos_sim", "cos_sim"],
    nneg=50,
    use_train_qrels=False,
).run()
```
```bash
# Train the gpl model
python -m gpl.train \
--path_to_generated_data "/content/drive/MyDrive/UVA/Thesis/training/gpl/bsard2/generated" \
--base_ckpt "camembert-base" \
--gpl_score_function "dot" \
--batch_size_gpl 32 \
--batch_size_generation 1 \
--gpl_steps 140000 \
--new_size -1 \
--queries_per_passage 1 \
--output_dir "/content/drive/MyDrive/UVA/Thesis/training/gpl/bsard2/output" \
--evaluation_data "/content/drive/MyDrive/UVA/Thesis/training/gpl/bsard2" \
--evaluation_output "/content/drive/MyDrive/UVA/Thesis/training/gpl/bsard2/evaluation" \
--generator "doc2query/msmarco-french-mt5-base-v1" \
--retrievers "antoinelouis/biencoder-camembert-base-mmarcoFR" "antoinelouis/biencoder-mMiniLMv2-L12-mmarcoFR" \
--retriever_score_functions "cos_sim" "cos_sim" \
--cross_encoder "antoinelouis/crossencoder-mMiniLMv2-L12-mmarcoFR" \
--qgen_prefix "qgen" \
--use_amp   # Use this for efficient training if the machine supports AMP
```
```python
# Evaluation the model
step = 20000
gpl.toolkit.evaluate(
    data_path = "/content/drive/MyDrive/UVA/Thesis/training/gpl/bsard2",
    output_dir = f"/content/drive/MyDrive/UVA/Thesis/training/gpl/bsard2/evaluation/{step}",
    model_name_or_path = f"/content/drive/MyDrive/UVA/Thesis/training/gpl/bsard2/output/{step}",
    score_function = "dot",
    pooling = "cls",
    split = "test",
    k_values = [1, 3, 5, 10, 20, 100, 200, 500]
)
```

####  Full Supervised
```bash
# Train with human labeled data
!python scripts/baseline/bsard/experiments/train_biencoder.py
# Evaluation
!python scripts/baseline/bsard/experiments/test_biencoder.py --checkpoint_path "your model checkpoint_path"
```

### 1st Stage Results Reproduction

#### Synthesizing Data
```bash
!python scripts/prompts/gpt_generate.py \
--prompt scripts/prompts/bsard/{The_prompt_path} \
--save_folder /content/drive/MyDrive/UVA/Thesis/synthetic_data/{your_prompt_method_name} \
--corpus scripts/prompts/data/articles_fr.csv \
--key "xxxxxxxxx" \
--org_key "xxxxxxx" \
--frac "0.05" \
--iterations "20" \
--exclude_index ""
```

#### Training
```bash
def create_incremental_data(method, end, start=0, base_path='/content/drive/MyDrive/UVA/Thesis/synthetic_data', ):
  # List of CSV file paths
  file_paths = file_paths = [
        f"{base_path}/{method}/train-{i}.csv"
        for i in range(start, end + 1)
  ]

  # Initialize an empty list to store DataFrames
  dfs = []

  # Loop through the file paths and read each file
  for file_path in file_paths:
      df = pd.read_csv(file_path)
      dfs.append(df)

  # Concatenate all DataFrames in the list into a single DataFrame
  combined_df = pd.concat(dfs, ignore_index=True)

  combined_df[["synthetic_question", "article_ids"]].to_csv(f"/content/drive/MyDrive/UVA/Thesis/synthetic_data/{method}/train-incremental.csv")

# Define your variables
end = 19
method = 'simple-ask'
output_suffix = end/10
model = "camembert-base"
output_path = f"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/{output_suffix}"
queries_filepath = f"/content/drive/MyDrive/UVA/Thesis/synthetic_data/{method}/train-incremental.csv"
epochs = 90

create_incremental_data(method=method,end=end)

!python scripts/baseline/bsard/experiments/train_biencoder_syn.py \
  --model {model} \
  --output_path {output_path} \
  --queries_filepath {queries_filepath} \
  --epochs {epochs}
```

#### Evaluation
```bash
epoch_list = [9,19,29,39,49,59,69,79,89]
for epoch in epoch_list:
  !python scripts/baseline/bsard/experiments/test_biencoder.py --checkpoint_path {your checkpoint_path}
```


### 2nd Stage Results Reproduction

```python
import pandas as pd
import json

df_articles = pd.read_csv('scripts/baseline/bsard/data/articles_fr.csv')
test_queries_df_path = 'scripts/baseline/bsard/data/questions_fr_validation_step_by_step_frac_4_6.csv'
df_validation = pd.read_csv(test_queries_df_path)
variation = 5

def train(current_round, epochs):
  print(f"### Starting training. Current Progress: {current_round} rounds / {target_round} rounds")
  # Define your variables
  end = 10 # start with 50% seed data
  previous_round = current_round - 1
  model = "camembert-base" # reinit parameters every round
  method = 'describe_then_ask_qcc_fewshots'
  output_path = f"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}"
  queries_filepath = f"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}/train.csv"

  if current_round == 0:
    # first iteration
    create_incremental_data(method=method,end=end)
    !cp /content/drive/MyDrive/UVA/Thesis/synthetic_data/{method}/train-incremental.csv /content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}/train.csv

  df = pd.read_csv(queries_filepath)
  df["synthetic_question"] = df["synthetic_question"].str.replace("[context] ", '', regex=False).str.replace("[question] ", '', regex=False)
  df["synthetic_question"] = df["synthetic_question"].str.replace("[Context] ", '', regex=False).str.replace("[Question] ", '', regex=False)
  df["synthetic_question"] = df["synthetic_question"].str.replace("[contexte] ", '', regex=False)
  df["synthetic_question"] = df["synthetic_question"].str.replace("[Contexte] ", '', regex=False)
  df["synthetic_question"] = df["synthetic_question"].str.replace("Contexte : ", '', regex=False)
  df["synthetic_question"] = df["synthetic_question"].str.replace("Situation :", '', regex=False)
  df["synthetic_question"] = df["synthetic_question"].str.replace("Question :", '', regex=False)
  df[["synthetic_question", "article_ids"]].to_csv(queries_filepath)

  !python scripts/baseline/bsard/experiments/train_biencoder_syn_step_by_step.py \
    --model {model} \
    --output_path {output_path} \
    --queries_filepath {queries_filepath} \
    --epochs {epochs}


def evaluate(current_round, epochs):
  print(f"### Starting evaluation. Current Progress: {current_round} rounds / {target_round} rounds")
  # Define your variables
  end = 2 # start with 10% seed data
  previous_round = current_round - 1
  model = "camembert-base" # reinit parameters every round
  method = 'describe_then_ask_qcc_fewshots'
  output_path = f"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}"
  queries_filepath = f"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}/train.csv"

  # evaluation on validation data
  !python scripts/baseline/bsard/experiments/test_biencoder_step_by_step.py \
  --checkpoint_path /content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round} \
  --test_queries_df_path {test_queries_df_path}
  # evaluation on test data
  !python scripts/baseline/bsard/experiments/test_biencoder.py --checkpoint_path /content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}


def prepare_for_extrapolation(current_round):
  print(f"### Preparing for extrapolation. Current Progress: {current_round} rounds / {target_round} rounds")
  method = 'describe_then_ask_qcc_fewshots'
  previous_round= current_round - 1
  top_k = 10 # only take top 10 wrongly retrieved pairs (can be ablated)

  wrong_pairs_to_extrapolate_file = f'/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{previous_round}/wrong_pairs_to_extrapolate.json'
  top_k_wrong_pairs = {}

  # read wrong pairs file
  with open(wrong_pairs_to_extrapolate_file) as json_file:
      wrong_pairs = json.load(json_file)

  # keep top_k article_ids
  for query_id, doc_ids in wrong_pairs.items():
    query = df_validation[df_validation['id'] == int(query_id)]['question'].values[0]
    articles = []
    for doc_id in doc_ids:
        article = df_articles[df_articles['id'] == int(doc_id)]['article'].values[0]
        if len(article.split(" ")) > 20:
          articles.append(doc_id)
        if len(articles) >= top_k:
          break
    top_k_wrong_pairs[query] = articles

  top_k_wrong_pairs = [(key, value) for key, values in top_k_wrong_pairs.items() for value in values]
  df_wrong_pairs = pd.DataFrame(top_k_wrong_pairs, columns=['Question', 'Article_Id'])

  def update_article(row):
      doc_id = row["Article_Id"]
      article = df_articles[df_articles['id'] == int(doc_id)]['article'].values[0]
      row["Article"] = article
      return row

  df_wrong_pairs = df_wrong_pairs.apply(update_article, axis=1)
  df_wrong_pairs.to_csv(f"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{previous_round}/wrong_pairs_to_extrapolate.csv")

def extrapolate(current_round):
  print(f"### Extrapolate wrong pairs for current round. Current Progress: {current_round} rounds / {target_round} rounds")
  method = 'describe_then_ask_qcc_fewshots'
  previous_round = current_round - 1
  !mkdir -p /content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}
  !python scripts/prompts/gpt_generate_extrapolate.py \
  --prompt scripts/prompts/bsard/extrapolate.txt \
  --corpus /content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{previous_round}/wrong_pairs_to_extrapolate.csv \
  --save_folder /content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round} \
  --key "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" \
  --org_key "org-ViWmGBWyZw44MQvxg2djVAff"

def merge_data(current_round):
  print(f"### Merging newly generated data to previous round data. Current Progress: {current_round} rounds / {target_round} rounds")
  method = 'describe_then_ask_qcc_fewshots'
  previous_round = current_round - 1
  file_paths = file_paths = [
        f"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}/extrapolated_queries_filtered.csv",
        f"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{previous_round}/train.csv"
  ]
  # Initialize an empty list to store DataFrames
  dfs = []

  # Loop through the file paths and read each file
  for file_path in file_paths:
      df = pd.read_csv(file_path)
      dfs.append(df)

  # Concatenate all DataFrames in the list into a single DataFrame
  combined_df = pd.concat(dfs, ignore_index=True)
  combined_df[["synthetic_question", "article_ids"]].to_csv(f"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}/train.csv")
  print(f"### Merged into /content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}/train.csv")

def syntactic_filter(current_round, syntactic_topk, semantic_threshold):
  print(f"### Filtering questions for current round. Current Progress: {current_round} rounds / {target_round} rounds")
  method = 'describe_then_ask_qcc_fewshots'
  previous_round = current_round - 1
  !python scripts/prompts/syntactic_filter.py \
  --extrapolated_queries /content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}/extrapolated_queries.csv \
  --wrong_pairs_to_extrapolate /content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{previous_round}/wrong_pairs_to_extrapolate.csv \
  --save_folder /content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round} \
  --syntactic_topk {syntactic_topk} \
  --semantic_threshold {semantic_threshold}

epochs = 90
current_round = 0
target_round = 2

while True:
  try:
      train(current_round = current_round, epochs=epochs)
      evaluate(current_round = current_round, epochs=epochs)
      current_round += 1
      if current_round > target_round:
        break
      prepare_for_extrapolation(current_round = current_round)
      extrapolate(current_round = current_round)
      merge_data(current_round = current_round)
  except Exception as e:
      print(e)
      break
```