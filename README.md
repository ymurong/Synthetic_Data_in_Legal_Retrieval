# Syntactical Structure Matters: Enhancing French Statutory Article Retrieval with GPT-Engineered Synthetic Questions

The goal of this thesis is to enhance the french statutory article retrieval performance through optimized query generation in order to decrease the reliance on human provided labelled examples. The collection of large-scale, high-quality annotated datasets especially in closed domains like legal text is not only time-consuming but also entails significant expenses.  This has constituted a significant bottleneck for various legal IR and natural language processing (NLP) tasks.

For this, we aim to explore the impact of syntactical structure differences of synthetic questions on legal domain adaptation and provide guidelines on how to use LLMs for better statutory retrieval performance in a semi-supervised way. The central research question that is answered during this research
is as follows:

The project aims to answer the following research question: 

> RQ: How do syntactical characteristics of synthetic legal questions generated by large language models (LLMs) influence the accuracy of statutory article retrieval, and based on this impact, what specific guidelines can be established for optimizing the use of LLMs in pseudo-labeling in legal information retrieval domain?

    * SRQ1 What are the syntactical structure differences between current synthetic questions and real human questions ? 

    * SRQ2 What are the prompting techniques that can reduce the syntactical structure differences ? 

    * SRQ3 To what extent can syntactical structure impact the accuracy of statutory article retrieval ?

    * SRQ4 What are the prompting techniques that improve the accuracy of statutory article retrieval?

The specific contributions of the proposed research are as follows:

(1) We provide optimized french synthetic queries for the statutory article dense retrieval task.

(2) We provide semi-supervised benchmark in the usage of different prompting techniques on the statutory article dense retrieval task. 

(3) We provide guidelines on synthetic query generation through LLMs prompting in legal information retrieval.

## Project Folder Structure

This project has the following folder structure:


## Project Setup

### Windows
First, you need to install dependencies.
```bash
python3 -m venv venv
source  venv/Scripts/activate
pip install -r requirements.txt
```

> For windows, wheel files here: https://pypi.tuna.tsinghua.edu.cn/simple or https://www.lfd.uci.edu/~gohlke/pythonlibs/

### Mac
First, you need to install dependencies.
```bash
python3 -m venv venv
source  venv/Scripts/activate
pip install -r requirements.txt
```

Secondly, some sub dependencies are needed.
```bash
python -m spacy download fr_core_news_md
```

## EDA
* [Exploratory Data Analysis (Before Experimentation)](scripts/eda/Exploratory Data Analysis.ipynb)
* [(WIP) Exploratory Data Analysis (After Experimentation)](scripts/eda/Exploratory Data Analysis(with openai).ipynb)


## Methodology




### Baseline


### Analysis and Experimentation


### Evaluation

Evaluation would be done on the labeled pairs of BSARD dataset so that we would be able to compare our results in order to address our sub research questions of SRQ1, SRQ2 and SRQ3. As presented in Table \ref{table:evaluation}, three standard information retrieval metrics \cite{schutze2008introduction} are used to evaluate performance, namely the (macro-averaged) recall@k (R@k), mean average precision@k (MAP@k), and mean reciprocal rank@k (MRR@k). We have selected the same value of k to enable a comparison of our results with those presented in the BSARD paper.


## Experiments

### Lexical Models

In order to reproduce the TF-IDF and BM25 models, run:
```bash
python scripts/baseline/bsard/experiments/run_zeroshot_evaluation.py \
    --retriever {tfidf, bm25} \ 
    --lem true
```

### Train Dense Model
```bash
python scripts/baseline/bsard/experiments/train_biencoder.py
```


