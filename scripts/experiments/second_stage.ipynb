{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T18:05:52.551238800Z",
     "start_time": "2024-05-18T18:05:51.967318900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Common Functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc28614718156b0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_incremental_data(method, end, start=0, base_path='/content/drive/MyDrive/UVA/Thesis/synthetic_data'):\n",
    "    # List of CSV file paths\n",
    "    file_paths = file_paths = [\n",
    "        f\"{base_path}/{method}/train-{i}.csv\"\n",
    "        for i in range(start, end + 1)\n",
    "    ]\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Loop through the file paths and read each file\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames in the list into a single DataFrame\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    combined_df[[\"synthetic_question\", \"article_ids\"]].to_csv(\n",
    "        f\"/content/drive/MyDrive/UVA/Thesis/synthetic_data/{method}/train-incremental.csv\")\n",
    "\n",
    "\n",
    "def train(current_round, epochs, target_round, end=10):\n",
    "    print(f\"### Starting training. Current Progress: {current_round} rounds / {target_round} rounds\")\n",
    "    # Define your variables\n",
    "    model = \"camembert-base\"  # reinit parameters every round\n",
    "    method = 'describe_then_ask_qcc_fewshots'\n",
    "    output_path = f\"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}\"\n",
    "    queries_filepath = f\"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}/train.csv\"\n",
    "\n",
    "    if current_round == 0:\n",
    "        # first iteration\n",
    "        create_incremental_data(method=method, end=end)\n",
    "        !cp/content/drive/MyDrive/UVA/Thesis/synthetic_data/ {method} /train-incremental.csv/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/ {method} /step_by_step{variation} / {current_round} /train.csv\n",
    "\n",
    "    df = pd.read_csv(queries_filepath)\n",
    "    df[\"synthetic_question\"] = df[\"synthetic_question\"].str.replace(\"[context] \", '', regex=False).str.replace(\n",
    "        \"[question] \", '', regex=False)\n",
    "    df[\"synthetic_question\"] = df[\"synthetic_question\"].str.replace(\"[Context] \", '', regex=False).str.replace(\n",
    "        \"[Question] \", '', regex=False)\n",
    "    df[\"synthetic_question\"] = df[\"synthetic_question\"].str.replace(\"[contexte] \", '', regex=False)\n",
    "    df[\"synthetic_question\"] = df[\"synthetic_question\"].str.replace(\"[Contexte] \", '', regex=False)\n",
    "    df[\"synthetic_question\"] = df[\"synthetic_question\"].str.replace(\"Contexte : \", '', regex=False)\n",
    "    df[\"synthetic_question\"] = df[\"synthetic_question\"].str.replace(\"Situation :\", '', regex=False)\n",
    "    df[\"synthetic_question\"] = df[\"synthetic_question\"].str.replace(\"Question :\", '', regex=False)\n",
    "    df[[\"synthetic_question\", \"article_ids\"]].to_csv(queries_filepath)\n",
    "\n",
    "    !python scripts/baseline/bsard/experiments/train_biencoder_syn_step_by_step.py \\\n",
    "            --model {model} \\\n",
    "            --output_path {output_path} \\\n",
    "            --queries_filepath {queries_filepath} \\\n",
    "            --epochs {epochs}\n",
    "\n",
    "\n",
    "def evaluate(current_round, epochs, target_round):\n",
    "    print(f\"### Starting evaluation. Current Progress: {current_round} rounds / {target_round} rounds\")\n",
    "    # Define your variables\n",
    "    model = \"camembert-base\"  # reinit parameters every round\n",
    "    method = 'describe_then_ask_qcc_fewshots'\n",
    "    output_path = f\"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}\"\n",
    "    queries_filepath = f\"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}/train.csv\"\n",
    "\n",
    "    # evaluation on validation data\n",
    "    !python scripts/baseline/bsard/experiments/test_biencoder_step_by_step.py \\\n",
    "            --checkpoint_path/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/ {method} /step_by_step{variation} / {current_round} \\\n",
    "            --test_queries_df_path {test_queries_df_path}\n",
    "    # evaluation on test data\n",
    "    !python scripts/baseline/bsard/experiments/test_biencoder.py --checkpoint_path/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/ {method} /step_by_step{variation} / {current_round}\n",
    "\n",
    "\n",
    "def prepare_for_extrapolation(current_round, target_round, method, wrong_top_k):\n",
    "    print(f\"### Preparing for extrapolation. Current Progress: {current_round} rounds / {target_round} rounds\")\n",
    "    previous_round = current_round - 1\n",
    "\n",
    "    wrong_pairs_to_extrapolate_file = f'/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{previous_round}/wrong_pairs_to_extrapolate.json'\n",
    "    top_k_wrong_pairs = {}\n",
    "\n",
    "    # read wrong pairs file\n",
    "    with open(wrong_pairs_to_extrapolate_file) as json_file:\n",
    "        wrong_pairs = json.load(json_file)\n",
    "\n",
    "    # keep top_k article_ids\n",
    "    for query_id, doc_ids in wrong_pairs.items():\n",
    "        query = df_validation[df_validation['id'] == int(query_id)]['question'].values[0]\n",
    "        articles = []\n",
    "        for doc_id in doc_ids:\n",
    "            article = df_articles[df_articles['id'] == int(doc_id)]['article'].values[0]\n",
    "            if len(article.split(\" \")) > 20:\n",
    "                articles.append(doc_id)\n",
    "            if len(articles) >= wrong_top_k:\n",
    "                break\n",
    "        top_k_wrong_pairs[query] = articles\n",
    "\n",
    "    top_k_wrong_pairs = [(key, value) for key, values in top_k_wrong_pairs.items() for value in values]\n",
    "    df_wrong_pairs = pd.DataFrame(top_k_wrong_pairs, columns=['Question', 'Article_Id'])\n",
    "\n",
    "    def update_article(row):\n",
    "        doc_id = row[\"Article_Id\"]\n",
    "        article = df_articles[df_articles['id'] == int(doc_id)]['article'].values[0]\n",
    "        row[\"Article\"] = article\n",
    "        return row\n",
    "\n",
    "    df_wrong_pairs = df_wrong_pairs.apply(update_article, axis=1)\n",
    "    df_wrong_pairs.to_csv(\n",
    "        f\"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{previous_round}/wrong_pairs_to_extrapolate.csv\")\n",
    "\n",
    "\n",
    "def extrapolate(current_round, target_round, method):\n",
    "    print(\n",
    "        f\"### Extrapolate wrong pairs for current round. Current Progress: {current_round} rounds / {target_round} rounds\")\n",
    "    previous_round = current_round - 1\n",
    "    !mkdir -p/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/ {method} /step_by_step{variation} / {current_round}\n",
    "    !python scripts/prompts/gpt_generate_extrapolate.py \\\n",
    "            --prompt scripts/prompts/bsard/extrapolate.txt \\\n",
    "            --corpus/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/ {method} /step_by_step{variation} / {previous_round} /wrong_pairs_to_extrapolate.csv \\\n",
    "            --save_folder/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/ {method} /step_by_step{variation} / {current_round} \\\n",
    "            --key \"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\" \\\n",
    "            --org_key \"org-ViWmGBWyZw44MQvxg2djVAff\"\n",
    "\n",
    "\n",
    "def merge_data(current_round, target_round):\n",
    "    print(\n",
    "        f\"### Merging newly generated data to previous round data. Current Progress: {current_round} rounds / {target_round} rounds\")\n",
    "    method = 'describe_then_ask_qcc_fewshots'\n",
    "    previous_round = current_round - 1\n",
    "    file_paths = file_paths = [\n",
    "        f\"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}/extrapolated_queries_filtered.csv\",\n",
    "        f\"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{previous_round}/train.csv\"\n",
    "    ]\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Loop through the file paths and read each file\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames in the list into a single DataFrame\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    combined_df[[\"synthetic_question\", \"article_ids\"]].to_csv(\n",
    "        f\"/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}/train.csv\")\n",
    "    print(\n",
    "        f\"### Merged into /content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/{method}/step_by_step{variation}/{current_round}/train.csv\")\n",
    "\n",
    "\n",
    "def syntactic_filter(current_round, method, syntactic_topk, semantic_threshold, random=False):\n",
    "    print(\n",
    "        f\"### Filtering questions for current round. Current Progress: {current_round} rounds / {target_round} rounds\")\n",
    "    previous_round = current_round - 1\n",
    "    !python scripts/prompts/syntactic_filter.py \\\n",
    "            --extrapolated_queries/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/ {method} /step_by_step{variation} / {current_round} /extrapolated_queries.csv \\\n",
    "            --wrong_pairs_to_extrapolate/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/ {method} /step_by_step{variation} / {previous_round} /wrong_pairs_to_extrapolate.csv \\\n",
    "            --save_folder/content/drive/MyDrive/UVA/Thesis/training/synthesizing/output/ {method} /step_by_step{variation} / {current_round} \\\n",
    "            --syntactic_topk {syntactic_topk} \\\n",
    "            --semantic_threshold {semantic_threshold} \\\n",
    "            --random {random}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d86720e7f7358ae8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training and Evaluation (Extrapolation without Syntactic filter)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24625fa69956259d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 90\n",
    "current_round = 0\n",
    "target_round = 2\n",
    "end = 10  # start with 50% seed data\n",
    "wrong_top_k = 10  # only take top 10 wrongly retrieved pairs (can be ablated)\n",
    "method = 'describe_then_ask_qcc_fewshots'\n",
    "df_articles = pd.read_csv('scripts/baseline/bsard/data/articles_fr.csv')\n",
    "test_queries_df_path = 'scripts/baseline/bsard/data/questions_fr_validation_step_by_step_frac_4_6.csv'\n",
    "df_validation = pd.read_csv(test_queries_df_path)\n",
    "variation = 5\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        train(current_round=current_round, target_round=target_round, epochs=epochs, end=end)\n",
    "        evaluate(current_round=current_round, target_round=target_round, epochs=epochs)\n",
    "        current_round += 1\n",
    "        if current_round > target_round:\n",
    "            break\n",
    "        prepare_for_extrapolation(current_round=current_round, target_round=target_round, method=method, wrong_top_k=wrong_top_k)\n",
    "        extrapolate(current_round=current_round, target_round=target_round, method=method)\n",
    "        merge_data(current_round=current_round, target_round=target_round)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "176f1e8075eecafc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training and Evaluation (Extrapolation with Syntactic filter)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "540df41bf036655f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 90\n",
    "current_round = 1\n",
    "target_round = 1\n",
    "method = 'describe_then_ask_qcc_fewshots'\n",
    "syntactic_topk = 1000\n",
    "semantic_threshold = 0.8\n",
    "random = True\n",
    "\n",
    "syntactic_filter(current_round=current_round, method=method, syntactic_topk=syntactic_topk,\n",
    "                 semantic_threshold=semantic_threshold, random=random)\n",
    "merge_data(current_round=current_round, target_round=target_round)\n",
    "train(current_round=current_round, epochs=epochs, target_round=target_round)\n",
    "evaluate(current_round=current_round, epochs=epochs, target_round=target_round)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "246dd8a7a2a71eb8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
